{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## I.Introduction\n\n:label:`sec_bptt`\n\nIf you completed the exercises in :numref:`sec_rnn-scratch`,\nyou would have seen that gradient clipping is vital \nfor preventing the occasional massive gradients\nfrom destabilizing training.\nWe hinted that the exploding gradients\nstem from backpropagating across long sequences.\nBefore introducing a slew of modern RNN architectures,\nlet's take a closer look at how *backpropagation*\nworks in sequence models in mathematical detail.\nHopefully, this discussion will bring some precision \nto the notion of *vanishing* and *exploding* gradients.\nIf you recall our discussion of forward and backward \npropagation through computational graphs\nwhen we introduced MLPs in :numref:`sec_backprop`,\nthen forward propagation in RNNs\nshould be relatively straightforward.\nApplying backpropagation in RNNs \nis called *backpropagation through time* :cite:`Werbos.1990`.\nThis procedure requires us to expand (or unroll) \nthe computational graph of an RNN\none time step at a time.\nThe unrolled RNN is essentially \na feedforward neural network \nwith the special property \nthat the same parameters \nare repeated throughout the unrolled network,\nappearing at each time step.\nThen, just as in any feedforward neural network,\nwe can apply the chain rule, \nbackpropagating gradients through the unrolled net.\nThe gradient with respect to each parameter\nmust be summed across all places \nthat the parameter occurs in the unrolled net.\nHandling such weight tying should be familiar \nfrom our chapters on convolutional neural networks.\n\n\nComplications arise because sequences\ncan be rather long.\nIt is not unusual to work with text sequences\nconsisting of over a thousand tokens. \nNote that this poses problems both from \na computational (too much memory)\nand optimization (numerical instability)\nstandpoint. \nInput from the first step passes through\nover 1000 matrix products before arriving at the output, \nand another 1000 matrix products \nare required to compute the gradient. \nWe now analyze what can go wrong and \nhow to address it in practice.","metadata":{}},{"cell_type":"markdown","source":"##  My Note for Introduction\n\n*   **Importance of Gradient Clipping**: The content above highlight that **gradient clipping is vital** for preventing **occasional massive gradients** from destabilizing the training process of RNNs. You would have encountered this necessity in the exercises from Section 9.5. Gradient clipping is an **inelegant but ubiquitous solution** used to limit the size of exploding gradients by forcing them to take smaller values, thereby preventing large updates that can destabilize training and cause the neural network to diverge.\n\n*   **Origin of Exploding Gradients**: The text points out that **exploding gradients stem from backpropagating across long sequences**. In RNNs, the \"depth\" of the network isn't just about the number of layers in a single time step, but also the length of the sequence. An input at the first time step influences the output at the final time step by passing through a chain of `T` (sequence length) layers along the time steps. When computing gradients backwards, this results in a chain of matrix-products of length `O(T)`. Such long chains can lead to numerical instability, causing gradients to either explode or vanish, depending on the properties of the weight matrices.\n\n*   **Backpropagation Through Time (BPTT) Definition**:\n    *   **BPTT** is the name given to the process of **applying backpropagation in sequence models**. This method was introduced by Werbos in 1990.\n    *   To perform BPTT, the **computational graph of an RNN is expanded or \"unrolled\" one time step at a time**. This effectively transforms the recurrent network into what looks like a very **deep feedforward neural network**.\n    *   A unique characteristic of this unrolled network is that the **same parameters are repeated throughout**, appearing at each time step. This is known as **weight tying**.\n    *   Once unrolled, the standard **chain rule** is applied to backpropagate gradients through this expanded network, just as in any feedforward neural network.\n    *   A crucial step in BPTT is that the **gradient with respect to each parameter must be summed across all places that the parameter occurs in the unrolled net**. This summation accounts for the shared nature of the weights across time steps. This concept of shared parameters and summing gradients should be familiar from convolutional neural networks.\n\n*   **Precision to Vanishing and Exploding Gradients**: The detailed analysis of how backpropagation works mathematically in sequence models provides a precise understanding of the **vanishing and exploding gradient problems**. These problems arise because the gradient calculation for hidden layer weights involves a **recursive dependency** that, when expanded, includes a sum of terms involving products of derivatives from many past time steps. Specifically, in simplified linear models, this involves potentially very large powers of weight matrices ($\\mathbf{W}_\\textrm{hh}^\\top$). If the eigenvalues of these matrices are smaller than 1, gradients vanish; if larger than 1, they diverge, leading to numerical instability.\n\n\nWhen dealing with **long sequences** in Recurrent Neural Networks (RNNs), such as text sequences consisting of over a thousand tokens, two main categories of problems arise: **computational issues** (too much memory) and **optimization issues** (numerical instability).\n### Computational Problems (Too Much Memory)\n\n*   **Unrolling the Computational Graph**: In RNNs, applying backpropagation requires \"unrolling\" the computational graph of the network one time step at a time. This unrolled RNN can be thought of as a very deep feedforward neural network, where the same parameters are shared across all time steps (layers).\n*   **Memory Consumption**: For a sequence with over a thousand tokens, this unrolling creates a computational graph with thousands of effective \"layers\" or time steps. During backpropagation, **all intermediate activations and gradients at each time step must be stored in memory** to compute the final gradients. This can quickly lead to an **excessive memory requirement**, making training infeasible for very long sequences.\n\n### Optimization Problems (Numerical Instability: Exploding and Vanishing Gradients)\n\n*   **Backpropagation Through Time and Chain Rule**: When computing gradients in RNNs, especially with respect to the hidden layer weights ($w_h$ or $\\mathbf{W}_{\\text{hh}}$), the chain rule is applied across many time steps. The total derivative of the hidden state at time $t$ with respect to $w_h$ involves a sum of terms, each containing a **product of derivatives (Jacobians) across previous time steps**. For long sequences, this translates to a chain of **hundreds or thousands of matrix products** during the gradient computation.\n*   **Exploding Gradients**:\n    *   This occurs when the norm of the gradients ($\\|\\mathbf{g}\\|$) becomes **\"excessively large\"**.\n    *   Mathematically, this happens when the eigenvalues of the weight matrices (specifically $\\mathbf{W}_{\\text{hh}}^\\top$ in the simplified model) are **larger than 1** when raised to high powers.\n    *   When gradients explode, a single gradient step can be so drastic that it **\"could undo all of the progress made over the course of thousands of training iterations\"**.\n    *   This often leads to neural network training **diverging**, meaning it fails to reduce the objective function's value, or it converges in an **unstable manner with \"massive spikes in the loss\"**.\n*   **Vanishing Gradients**:\n    *   Although the query focuses on exploding gradients, the sources also highlight vanishing gradients as a fundamental problem arising from numerical instability in RNNs.\n    *   This occurs when the eigenvalues of the weight matrices are **smaller than 1** when raised to high powers, causing the gradients to become extremely small. When gradients vanish, the updates to parameters become negligible, effectively stopping the learning process, especially for long-term dependencies.\n\n### How to Address These Problems in Practice\n\n*   **Gradient Clipping (for Exploding Gradients)**:\n    *   This is an \"inelegant but ubiquitous solution\" to exploding gradients.\n    *   The core idea is to **limit the maximum norm of the gradient vector**. If the calculated gradient's norm exceeds a predefined threshold ($\\theta$), the gradient is scaled down proportionally so its norm equals $\\theta$. The formula for this is $\\mathbf{g} \\leftarrow \\min \\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}$.\n    *   This ensures that the gradient norm never exceeds $\\theta$ and maintains the direction of the original gradient, preventing drastically large updates that could destabilize training. It's considered a \"hack\" because it doesn't follow the true gradient, but it's very effective and widely adopted. The training process typically calculates gradients, then clips them, and finally updates model parameters using these clipped gradients.\n*   **Truncated Backpropagation Through Time (for both Memory and Instability)**:\n    *   This is the most common practical approach for handling long sequences.\n    *   Instead of backpropagating through the entire sequence, the sum of gradients in the chain rule is **truncated after a fixed number of steps** (e.g., `num_steps` or $\\tau$).\n    *   This means the model only considers a limited window of past observations for calculating gradients, thus **reducing the computational graph length** and **mitigating numerical instability** by limiting the number of matrix products in the chain.\n    *   While it's an \"approximation\" of the true gradient, it works well in practice and biases the model towards simpler and more stable learning by focusing on short-term influences.\n    *   This method is often referred to as \"regular truncation\" and is more commonly used than \"randomized truncation\" due to its practical advantages, despite the latter being theoretically more accurate in expectation.\n*   **More Sophisticated Architectures**:\n    *   Beyond these general techniques, more advanced RNN architectures, such as **Long Short-Term Memory (LSTM)** networks, were specifically designed to alleviate the vanishing gradient problem and improve the handling of long-term dependencies. These will be discussed in later sections of the sources.","metadata":{}},{"cell_type":"markdown","source":"## II.Analysis of Gradients in RNNs\n:label:`subsec_bptt_analysis`\n\nWe start with a simplified model of how an RNN works.\nThis model ignores details about the specifics \nof the hidden state and how it is updated.\nThe mathematical notation here\ndoes not explicitly distinguish\nscalars, vectors, and matrices.\nWe are just trying to develop some intuition.\nIn this simplified model,\nwe denote $h_t$ as the hidden state,\n$x_t$ as input, and $o_t$ as output\nat time step $t$.\nRecall our discussions in\n:numref:`subsec_rnn_w_hidden_states`\nthat the input and the hidden state\ncan be concatenated before being multiplied \nby one weight variable in the hidden layer.\nThus, we use $w_\\textrm{h}$ and $w_\\textrm{o}$ to indicate the weights \nof the hidden layer and the output layer, respectively.\nAs a result, the hidden states and outputs \nat each time step are\n\n$$\\begin{aligned}h_t &= f(x_t, h_{t-1}, w_\\textrm{h}),\\\\o_t &= g(h_t, w_\\textrm{o}),\\end{aligned}$$\n:eqlabel:`eq_bptt_ht_ot`\n\nwhere $f$ and $g$ are transformations\nof the hidden layer and the output layer, respectively.\nHence, we have a chain of values \n$\\{\\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \\ldots\\}$ \nthat depend on each other via recurrent computation.\nThe forward propagation is fairly straightforward.\nAll we need is to loop through the $(x_t, h_t, o_t)$ triples one time step at a time.\nThe discrepancy between output $o_t$ and the desired target $y_t$ \nis then evaluated by an objective function \nacross all the $T$ time steps as\n\n$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_\\textrm{h}, w_\\textrm{o}) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t).$$\n\n\n\nFor backpropagation, matters are a bit trickier, \nespecially when we compute the gradients \nwith regard to the parameters $w_\\textrm{h}$ of the objective function $L$. \nTo be specific, by the chain rule,\n\n$$\\begin{aligned}\\frac{\\partial L}{\\partial w_\\textrm{h}}  & = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_\\textrm{h}}  \\\\& = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_\\textrm{o})}{\\partial h_t}  \\frac{\\partial h_t}{\\partial w_\\textrm{h}}.\\end{aligned}$$\n:eqlabel:`eq_bptt_partial_L_wh`\n\nThe first and the second factors of the\nproduct in :eqref:`eq_bptt_partial_L_wh`\nare easy to compute.\nThe third factor $\\partial h_t/\\partial w_\\textrm{h}$ is where things get tricky, \nsince we need to recurrently compute the effect of the parameter $w_\\textrm{h}$ on $h_t$.\nAccording to the recurrent computation\nin :eqref:`eq_bptt_ht_ot`,\n$h_t$ depends on both $h_{t-1}$ and $w_\\textrm{h}$,\nwhere computation of $h_{t-1}$\nalso depends on $w_\\textrm{h}$.\nThus, evaluating the total derivate of $h_t$ \nwith respect to $w_\\textrm{h}$ using the chain rule yields\n\n$$\\frac{\\partial h_t}{\\partial w_\\textrm{h}}= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}} +\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}.$$\n:eqlabel:`eq_bptt_partial_ht_wh_recur`\n\n\nTo derive the above gradient, assume that we have \nthree sequences $\\{a_{t}\\},\\{b_{t}\\},\\{c_{t}\\}$ \nsatisfying $a_{0}=0$ and $a_{t}=b_{t}+c_{t}a_{t-1}$ for $t=1, 2,\\ldots$.\nThen for $t\\geq 1$, it is easy to show\n\n$$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}.$$\n:eqlabel:`eq_bptt_at`\n\nBy substituting $a_t$, $b_t$, and $c_t$ according to\n\n$$\\begin{aligned}a_t &= \\frac{\\partial h_t}{\\partial w_\\textrm{h}},\\\\\nb_t &= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}, \\\\\nc_t &= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}},\\end{aligned}$$\n\nthe gradient computation in :eqref:`eq_bptt_partial_ht_wh_recur` satisfies\n$a_{t}=b_{t}+c_{t}a_{t-1}$.\nThus, per :eqref:`eq_bptt_at`, \nwe can remove the recurrent computation \nin :eqref:`eq_bptt_partial_ht_wh_recur` with\n\n$$\\frac{\\partial h_t}{\\partial w_\\textrm{h}}=\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_\\textrm{h})}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_{i},h_{i-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}.$$\n:eqlabel:`eq_bptt_partial_ht_wh_gen`\n\nWhile we can use the chain rule to compute $\\partial h_t/\\partial w_\\textrm{h}$ recursively, \nthis chain can get very long whenever $t$ is large.\nLet's discuss a number of strategies for dealing with this problem.\n\n### Full Computation ### \n\nOne idea might be to compute the full sum in :eqref:`eq_bptt_partial_ht_wh_gen`.\nHowever, this is very slow and gradients can blow up,\nsince subtle changes in the initial conditions\ncan potentially affect the outcome a lot.\nThat is, we could see things similar to the butterfly effect,\nwhere minimal changes in the initial conditions \nlead to disproportionate changes in the outcome.\nThis is generally undesirable.\nAfter all, we are looking for robust estimators that generalize well. \nHence this strategy is almost never used in practice.\n\n### Truncating Time Steps###\n\nAlternatively,\nwe can truncate the sum in\n:eqref:`eq_bptt_partial_ht_wh_gen`\nafter $\\tau$ steps. \nThis is what we have been discussing so far. \nThis leads to an *approximation* of the true gradient,\nsimply by terminating the sum at $\\partial h_{t-\\tau}/\\partial w_\\textrm{h}$. \nIn practice this works quite well. \nIt is what is commonly referred to as truncated \nbackpropgation through time :cite:`Jaeger.2002`.\nOne of the consequences of this is that the model \nfocuses primarily on short-term influence \nrather than long-term consequences. \nThis is actually *desirable*, since it biases the estimate \ntowards simpler and more stable models.\n\n\n### Randomized Truncation ### \n\nLast, we can replace $\\partial h_t/\\partial w_\\textrm{h}$\nby a random variable which is correct in expectation \nbut truncates the sequence.\nThis is achieved by using a sequence of $\\xi_t$\nwith predefined $0 \\leq \\pi_t \\leq 1$,\nwhere $P(\\xi_t = 0) = 1-\\pi_t$ and \n$P(\\xi_t = \\pi_t^{-1}) = \\pi_t$, thus $E[\\xi_t] = 1$.\nWe use this to replace the gradient\n$\\partial h_t/\\partial w_\\textrm{h}$\nin :eqref:`eq_bptt_partial_ht_wh_recur`\nwith\n\n$$z_t= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}.$$\n\n\nIt follows from the definition of $\\xi_t$ \nthat $E[z_t] = \\partial h_t/\\partial w_\\textrm{h}$.\nWhenever $\\xi_t = 0$ the recurrent computation\nterminates at that time step $t$.\nThis leads to a weighted sum of sequences of varying lengths,\nwhere long sequences are rare but appropriately overweighted. \nThis idea was proposed by \n:citet:`Tallec.Ollivier.2017`.\n\n### Comparing Strategies\n\n![Comparing strategies for computing gradients in RNNs. From top to bottom: randomized truncation, regular truncation, and full computation.](../img/truncated-bptt.svg)\n:label:`fig_truncated_bptt`\n\n\n:numref:`fig_truncated_bptt` illustrates the three strategies \nwhen analyzing the first few characters of *The Time Machine* \nusing backpropagation through time for RNNs:\n\n* The first row is the randomized truncation that partitions the text into segments of varying lengths.\n* The second row is the regular truncation that breaks the text into subsequences of the same length. This is what we have been doing in RNN experiments.\n* The third row is the full backpropagation through time that leads to a computationally infeasible expression.\n\n\nUnfortunately, while appealing in theory, \nrandomized truncation does not work \nmuch better than regular truncation, \nmost likely due to a number of factors.\nFirst, the effect of an observation\nafter a number of backpropagation steps \ninto the past is quite sufficient \nto capture dependencies in practice. \nSecond, the increased variance counteracts the fact \nthat the gradient is more accurate with more steps. \nThird, we actually *want* models that have only \na short range of interactions. \nHence, regularly truncated backpropagation through time \nhas a slight regularizing effect that can be desirable.\n\n","metadata":{}},{"cell_type":"markdown","source":"## My Note For 9.7.1\n### Simplified RNN Model Components\nIn this simplified model, at each time step $t$:\n*   **$h_t$** denotes the **hidden state**.\n*   **$x_t$** represents the **input**.\n*   **$o_t$** signifies the **output**.\n*   **$w_h$** indicates the **weights of the hidden layer**.\n*   **$w_o$** indicates the **weights of the output layer**.\n\nThe sources remind us that the input and hidden state can be concatenated before being multiplied by a single weight variable in the hidden layer, which $w_h$ broadly represents here.\n\n### Formula (9.7.1): Hidden State and Output Computation\nThe core of the RNN's recurrent nature is captured by how its hidden state and output are computed at each time step. Formula (9.7.1) expresses these computations:\n\n**H*t* = ùúô(X*t*Wxh + H*t*-1Whh + bh)**\n**O*t* = H*t*Whq + bq**\n\nIn the simplified notation of Section 9.7.1, these become:\n*   **$h_t = f(x_t, h_{t-1}, w_h)$**\n    *   This equation defines how the **hidden state at the current time step $t$ ($h_t$) is calculated**.\n    *   It depends on three main components:\n        *   The **current input $x_t$**: The information being fed into the network at this specific time step.\n        *   The **previous hidden state $h_{t-1}$**: This is the \"memory\" or \"state\" of the network from the preceding time step. Its inclusion is what makes RNNs \"recurrent\" and allows them to capture dependencies over time.\n        *   The **hidden layer weights $w_h$**: These are the parameters that the model learns to transform the input and previous hidden state into the new hidden state. The function $f$ represents this transformation, including any activation functions (like tanh, as used in the `RNNScratch` implementation) and the combination of input-to-hidden and hidden-to-hidden weights. As discussed previously, the hidden state update combines the current input and the previous hidden state, passing the result through an activation function.\n*   **$o_t = g(h_t, w_o)$**\n    *   This equation defines how the **output at the current time step $t$ ($o_t$) is calculated**.\n    *   It depends on:\n        *   The **current hidden state $h_t$**: The processed information up to time step $t$ is used to generate the output for this step.\n        *   The **output layer weights $w_o$**: These are the parameters that transform the hidden state into the final output. The function $g$ represents this transformation, analogous to how $H_t$ is multiplied by $W_{hq}$ and summed with $b_q$ in a more detailed RNN definition.\n\nTogether, these equations illustrate a **chain of values** $\\{\\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_t, h_t, o_t), \\ldots\\}$ where each triplet depends on the previous one via recurrent computation. This dependency structure is key to how RNNs process sequential data, where elements are related over time.\n\n### Formula (9.7.2): Objective Function (Loss Calculation)\nAfter the forward propagation, where outputs $o_t$ are generated for each time step, the model's performance is evaluated using an objective function (or loss function). Formula (9.7.2) details this calculation:\n\n*   **$L (x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_h, w_o) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t)$**\n    *   This represents the **total objective function $L$**, which is the **average loss over all $T$ time steps** of the sequence.\n    *   **Purpose**: It quantifies the \"discrepancy\" between the model's predicted outputs ($o_t$) and the desired target values ($y_t$) across the entire sequence.\n    *   **Summation $\\sum_{t=1}^T$**: The total loss is computed by summing the individual losses from each time step $t=1$ to $T$.\n    *   **Individual Loss $l(y_t, o_t)$**: This denotes the loss incurred at a single time step $t$, based on the predicted output $o_t$ and the true target $y_t$. Common choices for $l$ include cross-entropy loss for classification tasks.\n    *   **Averaging $\\frac{1}{T}$**: The sum of individual losses is divided by the total number of time steps $T$. This averaging makes the loss comparable across sequences of different lengths, similar to how perplexity normalizes likelihoods in language models.\n\nThese formulas lay the groundwork for understanding how gradients are computed and backpropagated through time in RNNs, which becomes trickier for parameters like $w_h$ due to their recurrent dependency on previous hidden states. The \"long chains of dependencies\" are what lead to the challenges of vanishing and exploding gradients, prompting the need for techniques like truncated BPTT.\n\n### Formula (9.7.3): Gradient of Total Loss with Respect to Hidden Weights\nTo understand how the model learns, we need to compute the gradient of the total objective function $L$ with respect to the hidden layer weights $w_h$. Since $w_h$ is shared across all time steps in an RNN, its influence on the total loss must be summed up from each time step where it contributes. Using the **chain rule** and summing across all time steps, the gradient is given by:\n\n**$\\frac{\\partial L}{\\partial w_\\textrm{h}} = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_\\textrm{h}} = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_\\textrm{o})}{\\partial h_t} \\frac{\\partial h_t}{\\partial w_\\textrm{h}}$**\n\n**Why this formula?**\n*   **Summation over $T$**: The total loss $L$ is a sum over all time steps, so by linearity of differentiation, the derivative of $L$ with respect to $w_h$ is the sum of derivatives of each $l(y_t, o_t)$ with respect to $w_h$. The $1/T$ factor is from the averaging in the objective function.\n*   **Chain Rule**: Each $l(y_t, o_t)$ depends on $o_t$, which depends on $h_t$, which in turn depends on $w_h$. The chain rule correctly accounts for these dependencies:\n    *   $\\frac{\\partial l(y_t, o_t)}{\\partial o_t}$: How the loss changes with respect to the output at time $t$. This is typically straightforward to compute (e.g., derivative of cross-entropy).\n    *   $\\frac{\\partial g(h_t, w_\\textrm{o})}{\\partial h_t}$: How the output $o_t$ changes with respect to the hidden state $h_t$. This is also generally straightforward, as $g$ is a simple transformation (e.g., a linear layer).\n    *   $\\frac{\\partial h_t}{\\partial w_\\textrm{h}}$: This is the **tricky part**. It represents how the current hidden state $h_t$ is affected by the hidden layer weights $w_h$. Its complexity arises because $h_t$ depends on $h_{t-1}$, and $h_{t-1}$ itself depends on $w_h$. This forms a **recurrent dependency**.\n\n### Formula (9.7.4): Recursive Gradient of Hidden State\nTo compute $\\frac{\\partial h_t}{\\partial w_\\textrm{h}}$, we again apply the chain rule, considering both the direct and indirect influences of $w_h$ on $h_t$:\n\n**$\\frac{\\partial h_t}{\\partial w_\\textrm{h}}= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}} +\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$**\n\n**Why this formula?**\n*   **First Term ($\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}$)**: This accounts for the **direct impact** of $w_h$ on $h_t$ at the current time step. It's the derivative of the transformation function $f$ with respect to $w_h$, assuming $h_{t-1}$ is a constant at this point.\n*   **Second Term ($\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$)**: This accounts for the **indirect impact** of $w_h$ on $h_t$ through the **previous hidden state $h_{t-1}$**.\n    *   $\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}}$: How the current hidden state $h_t$ changes with respect to the previous hidden state $h_{t-1}$. This is the \"recurrent connection\" part.\n    *   $\\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$: This is the crucial **recurrent term**. It means that to calculate $\\frac{\\partial h_t}{\\partial w_\\textrm{h}}$, you need to know $\\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$, which in turn needs $\\frac{\\partial h_{t-2}}{\\partial w_\\textrm{h}}$, and so on, all the way back to $h_0$ (which is typically initialized and doesn't depend on $w_h$). This recursive dependency is the core of BPTT.\n\n### Formulas (9.7.5) and (9.7.6): Unrolling the Recursion\nTo solve the recursive gradient calculation in (9.7.4), the sources introduce a general recurrence relation:\n*   Let $a_t = b_t + c_t a_{t-1}$ with $a_0 = 0$.\n*   This general form can be expanded to remove the recursion:\n    **$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}$**.\n\n**Why this helper?**\nThis mathematical identity provides a way to express a value $a_t$ that depends on $a_{t-1}$ as a sum of terms that only depend on $b_k$ and $c_k$ for $k \\le t$, effectively \"unrolling\" the recurrence.\n\nThen, the specific terms from our RNN gradient (9.7.4) are substituted into this general form:\n*   **$a_t = \\frac{\\partial h_t}{\\partial w_\\textrm{h}}$**\n*   **$b_t = \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}$**\n*   **$c_t = \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}}$**\n\nThis substitution explicitly links the general recursive solution to the specific gradient problem in RNNs.\n\n### Formula (9.7.7): Explicit Unrolled Gradient of Hidden State\nBy substituting $a_t, b_t, c_t$ back into the expanded form of $a_t$ from (9.7.5), we get the full, non-recursive expression for $\\frac{\\partial h_t}{\\partial w_\\textrm{h}}$:\n\n**$\\frac{\\partial h_t}{\\partial w_\\textrm{h}}=\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_\\textrm{h})}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_{i},h_{i-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}$**\n\n**Why this formula?**\n*   This formula **explicitly shows the \"backpropagation through time\"**. The derivative of $h_t$ with respect to $w_h$ depends on a sum of terms. Each term represents the influence of $w_h$ at a specific past time step $i$ on the current hidden state $h_t$.\n*   The product term $\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_\\textrm{h})}{\\partial h_{j-1}}$ is a chain of derivatives of $f$ with respect to its previous hidden state argument, effectively showing how the influence from time step $i$ propagates forward through all subsequent hidden states up to $t$.\n*   **The problem it highlights**: This long chain of multiplications, particularly the product of terms involving $\\frac{\\partial f(\\ldots)}{\\partial h_{j-1}}$, is the root cause of **vanishing and exploding gradients**. If these individual derivatives are consistently less than 1, the product can vanish to zero, leading to vanishing gradients. If they are consistently greater than 1, the product can explode, leading to exploding gradients.\n\n### Strategies for Addressing the Problem\n\nSince computing the full sum in (9.7.7) is \"very slow\" and prone to exploding gradients (likened to the \"butterfly effect\" where small changes disproportionately affect the outcome), it's \"almost never used in practice\". Instead, the sources discuss several strategies:\n\n*   **Truncating Time Steps (Regular Truncation)**: This is the most common practical approach where the sum in (9.7.7) is \"truncated after $\\tau$ steps\". This means backpropagation is only performed over a fixed, limited number of past time steps. This \"approximates the true gradient\" but \"works quite well in practice\". It biases the model towards focusing on \"short-term influence\" which can be desirable as it leads to \"simpler and more stable models\". This method also helps with memory consumption by limiting the length of the unrolled graph.\n\n*   **Randomized Truncation**: This approach replaces the exact gradient term $\\frac{\\partial h_t}{\\partial w_h}$ with a random variable $z_t$ that is \"correct in expectation\". It introduces a probability $\\pi_t$ for a given time step to terminate the recurrent computation ($P(\\xi_t = 0) = 1-\\pi_t$ and $P(\\xi_t = \\pi_t^{-1}) = \\pi_t$). This results in a weighted sum of sequences of varying lengths, where longer sequences are rarer but appropriately overweighted. Although theoretically appealing, randomized truncation \"does not work much better than regular truncation\" in practice, partly because the \"increased variance counteracts the fact that the gradient is more accurate with more steps\" and models often benefit from a \"short range of interactions\".\n\n*   **Gradient Clipping**: This is an \"inelegant but ubiquitous solution\" specifically for **exploding gradients**. It directly limits the \"maximum norm of the gradient vector\". If the computed gradient's norm exceeds a threshold $\\theta$, it's scaled down to $\\theta$ while preserving its direction. This prevents \"massive spikes in the loss\" and keeps training stable by preventing individual gradient steps from undoing progress. It's a \"hack\" because it doesn't follow the true gradient, but it's \"very useful\" and widely adopted.\n\n\nThese're three main strategies for handling **Backpropagation Through Time (BPTT)** in Recurrent Neural Networks (RNNs), specifically addressing the challenges that arise when computing gradients through long sequences. This challenge is rooted in the recurrent dependency of the hidden state $h_t$ on previous hidden states $h_{t-1}$ and the shared hidden layer weights $w_h$, as described by formula (9.7.4) and its unrolled form (9.7.7).\n\nThe core problem is that the term $\\frac{\\partial h_t}{\\partial w_\\textrm{h}}$ in the overall loss gradient (9.7.3) involves a recursive computation that, when fully expanded as in (9.7.7), becomes a **long chain of products**. This long chain can lead to **numerical instability**, causing gradients to either **explode** (become excessively large) or **vanish** (become negligibly small). The three strategies aim to mitigate these issues.\n\n### 9.7.1.1. Full Computation\n*   **What it is**: This strategy involves computing the **entire sum** as expressed in formula (9.7.7). This means considering the influence of the hidden layer weights $w_h$ from every single time step in the past ($i=1$ to $t-1$) on the current hidden state $h_t$.\n*   **Why it's problematic**:\n    *   **Computational Cost**: It is **very slow** because the chain can get \"very long whenever $t$ is large\". For sequences with thousands of tokens, this means calculating gradients through over a thousand matrix products.\n    *   **Numerical Instability**: Gradients can **blow up**. This is due to the \"high powers of matrices\" (specifically, powers of $\\mathbf{W}_\\textrm{hh}^\\top$ in the more detailed analysis in 9.7.2) which can lead to divergent eigenvalues, manifesting as exploding gradients.\n    *   **\"Butterfly Effect\"**: Subtle changes in initial conditions can disproportionately affect the outcome, making the model less robust.\n*   **Conclusion**: This strategy is **almost never used in practice** because it is computationally infeasible and numerically unstable.\n\n### 9.7.1.2. Truncating Time Steps\n*   **What it is**: This is the most common and practical approach, often referred to as **truncated backpropagation through time (BPTT)**. Instead of computing the full sum in (9.7.7) from $i=1$ to $t-1$, the sum is **truncated after $\\tau$ steps**. This means the backpropagation process only goes back a fixed number of time steps (e.g., $\\tau$ steps into the past), effectively terminating the sum at $\\frac{\\partial h_{t-\\tau}}{\\partial w_\\textrm{h}}$. It is an **approximation** of the true gradient.\n*   **How it works in practice**: This method breaks the text into subsequences of the same fixed length. This is what is commonly done in RNN experiments.\n*   **Why it's desirable**:\n    *   **Practicality**: It works **quite well in practice**.\n    *   **Focus on Short-Term Influence**: By truncating, the model primarily focuses on **short-term influence** rather than trying to capture very long-term dependencies. This can be desirable as it biases the estimate towards **simpler and more stable models**.\n    *   **Regularization**: It provides a **slight regularizing effect**.\n    *   **Mitigation of Instability**: It addresses the numerical instability by preventing the gradient chain from becoming excessively long, thus mitigating exploding gradients.\n\n### 9.7.1.3. Randomized Truncation\n*   **What it is**: This strategy proposes to replace the problematic gradient term $\\frac{\\partial h_t}{\\partial w_\\textrm{h}}$ (which leads to the long sum in 9.7.7) with a **random variable** $z_t$. The key idea is that this random variable $z_t$ is designed to be **correct in expectation**. This method was proposed by Tallec and Ollivier (2017).\n*   **Formula (9.7.8) explanation**:\n    $\\mathbf{z}_t = \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$.\n    *   This formula modifies the recursive gradient computation for $h_t$ from (9.7.4).\n    *   The term $\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}$ accounts for the direct impact of $w_h$ on $h_t$ at the current time step.\n    *   The crucial part is the inclusion of the random variable $\\xi_t$ in the second term. This $\\xi_t$ is chosen such that its expectation $E[\\xi_t] = 1$. Specifically, $\\xi_t$ can be $0$ with probability $1-\\pi_t$ or $\\pi_t^{-1}$ with probability $\\pi_t$ (where $0 \\leq \\pi_t \\leq 1$).\n    *   **Mechanism of Truncation**: Whenever $\\xi_t = 0$, the second term involving $\\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$ becomes zero, effectively **terminating the recurrent computation at that time step $t$**.\n    *   **Result**: This leads to a **weighted sum of sequences of varying lengths**. Longer sequences are rare but are \"appropriately overweighted\" (due to the $\\pi_t^{-1}$ factor when $\\xi_t \\neq 0$) to maintain the correct expectation of the gradient. The randomized truncation visually partitions the text into segments of varying lengths.\n*   **Comparison to Regular Truncation**:\n    *   Despite its theoretical appeal (being correct in expectation), randomized truncation **does not work much better than regular truncation in practice**.\n    *   This is attributed to several factors:\n        *   The effect of past observations is often sufficiently captured by a fixed number of backpropagation steps.\n        *   The **increased variance** introduced by the random truncation counteracts the benefit of a more accurate expected gradient.\n        *   The slight regularizing effect of regular truncation can be **desirable**.\n\nIn summary, all three strategies attempt to manage the computational and numerical challenges posed by backpropagating gradients through long sequences in RNNs. While full computation is theoretically ideal for accuracy, it's practically infeasible. Truncated BPTT offers a pragmatic and often desirable balance, while randomized truncation, though theoretically sound, hasn't shown significant practical advantages over regular truncation.","metadata":{}},{"cell_type":"markdown","source":"The section \"9.7.1.4. Comparing Strategies\" (also found in) directly addresses the practical implications and trade-offs of the three primary methods for handling **Backpropagation Through Time (BPTT)** in Recurrent Neural Networks (RNNs) that we've discussed: Full Computation, Truncating Time Steps (Regular Truncation), and Randomized Truncation. These strategies are crucial for overcoming the computational and numerical challenges that arise when computing gradients through long sequences in RNNs, where inputs from early time steps must pass through many matrix products to influence later outputs and gradients must traverse similarly long paths.\n\n### Visualizing the Strategies: Fig. 9.7.1\nThe provided **Fig. 9.7.1** visually illustrates how these three strategies differ in their approach to backpropagating gradients through a sequence:\n*   **The first row depicts Randomized Truncation**. It shows the text being partitioned into segments of *varying lengths*. This means the backpropagation path for gradients can randomly be cut short or extended, leading to sequences of different lengths being processed.\n*   **The second row shows Regular Truncation**. This method breaks the text into subsequences of the *same fixed length*. Gradients are then backpropagated only within these fixed-length segments. This is the common practice in RNN experiments.\n*   **The third row represents Full Backpropagation Through Time**. It illustrates a continuous, unbroken chain of dependencies extending across the entire sequence, highlighting why it leads to a **computationally infeasible expression** for very long sequences.\n\n### Detailed Comparison of Strategies\n\nThe comparison focuses on the practical effectiveness and underlying reasons for preferring one method over another:\n\n1.  **Full Computation**:\n    *   **Content**: This approach involves computing the **entire sum** of gradients for the hidden layer weights ($w_\\textrm{h}$) as given in formula (9.7.7). This means considering the influence of $w_\\textrm{h}$ from every single preceding time step on the current hidden state and its contribution to the loss.\n    *   **Why it's problematic**:\n        *   **Very slow**: The chain of dependencies in (9.7.7) \"can get very long whenever $t$ is large\", leading to immense computational cost and memory requirements.\n        *   **Numerical instability (Gradients blow up)**: As the chain of matrix products (specifically involving powers of $\\mathbf{W}_\\textrm{hh}^\\top$ in a more detailed model) grows, gradients can become excessively large, leading to **exploding gradients**. This can destabilize training and prevent convergence.\n        *   **\"Butterfly effect\"**: Subtle changes in initial conditions can disproportionately affect the outcome, making the model less robust and less likely to generalize well.\n    *   **Conclusion**: **This strategy is almost never used in practice** due to its computational impracticality and numerical instability.\n\n2.  **Truncating Time Steps (Regular Truncation)**:\n    *   **Content**: This is the most common and practical approach. Instead of the full sum, the gradient computation in (9.7.7) is **truncated after $\\tau$ steps**. This means the backpropagation process only goes back a fixed number of time steps into the past (e.g., $\\tau$ steps), effectively terminating the sum earlier. It is an **approximation** of the true gradient.\n    *   **Why it's desirable**:\n        *   **Practicality**: It \"works quite well in practice\".\n        *   **Mitigates instability**: By limiting the length of the gradient chain, it significantly reduces the problem of exploding gradients. While it doesn't solve vanishing gradients, more sophisticated architectures like LSTMs can help.\n        *   **Focus on short-term influence**: This truncation biases the model towards simpler and more stable models, focusing on short-term dependencies rather than attempting to capture very long-term consequences.\n        *   **Regularizing effect**: This approach has a \"slight regularizing effect that can be desirable\".\n\n3.  **Randomized Truncation**:\n    *   **Content**: This strategy aims to replace the problematic recurrent gradient term $\\frac{\\partial h_t}{\\partial w_\\textrm{h}}$ (as seen in formula 9.7.4) with a **random variable ($z_t$) that is correct in expectation** ($E[z_t] = \\frac{\\partial h_t}{\\partial w_\\textrm{h}}$).\n    *   **Formula (9.7.8) explained in context**:\n        *   The formula is $z_t= \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$.\n        *   The first term, $\\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial w_\\textrm{h}}$, represents the direct influence of $w_\\textrm{h}$ on $h_t$ at the current time step.\n        *   The second term, $\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_\\textrm{h})}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_\\textrm{h}}$, represents the recursive influence from the previous hidden state. The crucial part is the random variable $\\xi_t$.\n        *   **Mechanism**: $\\xi_t$ is chosen such that $P(\\xi_t = 0) = 1-\\pi_t$ and $P(\\xi_t = \\pi_t^{-1}) = \\pi_t$, where $0 \\leq \\pi_t \\leq 1$. This ensures that its expectation $E[\\xi_t] = 0 \\cdot (1-\\pi_t) + \\pi_t^{-1} \\cdot \\pi_t = 1$.\n        *   **Truncation effect**: **Whenever $\\xi_t = 0$, the second term becomes zero, and the recurrent computation for the gradient effectively terminates at that time step $t$**.\n        *   **Weighted sum**: This leads to a weighted sum of sequences of varying lengths. \"Long sequences are rare but appropriately overweighted\" (by the $\\pi_t^{-1}$ factor when $\\xi_t$ is not zero) to maintain the correct expectation of the gradient. This idea was proposed by Tallec and Ollivier (2017).\n\n### Why Randomized Truncation Doesn't Work Much Better in Practice\n\nDespite its theoretical appeal of providing an unbiased estimate of the full gradient, randomized truncation \"does not work much better than regular truncation\" in practice. This is attributed to several key factors:\n\n1.  **Sufficient Short-Term Dependencies**: The effect of an observation often diminishes rapidly as it goes further back in time. Thus, even with regular truncation, a fixed number of backpropagation steps (a \"short range of interactions\") is frequently **sufficient to capture dependencies** in practical scenarios.\n2.  **Increased Variance**: The randomness introduced by $\\xi_t$ in randomized truncation, while ensuring an unbiased expectation, **increases the variance** of the gradient estimates. This increased variance can counteract the benefit of a more accurate expected gradient, potentially leading to slower or less stable convergence in practice.\n3.  **Desirable Regularization**: Often, we \"actually *want* models that have only a short range of interactions\". The regular truncation, by its nature of limiting the dependency length, provides a **slight regularizing effect** that can be beneficial for model stability and generalization.\n\nIn conclusion, while full computation is theoretically ideal for gradient accuracy, it's practically unfeasible due to computational cost and numerical instability. Regular (truncated) backpropagation through time is a pragmatic and effective compromise, offering stability and computational efficiency while providing sufficient historical context for many tasks. Randomized truncation, despite its theoretical elegance, doesn't offer significant practical improvements, largely due to increased variance and the often-sufficient nature of short-term dependencies in real-world data.","metadata":{}},{"cell_type":"markdown","source":"## III.Backpropagation Through Time in Detail\n\nAfter discussing the general principle,\nlet's discuss backpropagation through time in detail.\nIn contrast to the analysis in :numref:`subsec_bptt_analysis`,\nin the following we will show how to compute\nthe gradients of the objective function\nwith respect to all the decomposed model parameters.\nTo keep things simple, we consider \nan RNN without bias parameters,\nwhose activation function in the hidden layer\nuses the identity mapping ($\\phi(x)=x$).\nFor time step $t$, let the single example input \nand the target be $\\mathbf{x}_t \\in \\mathbb{R}^d$ and $y_t$, respectively. \nThe hidden state $\\mathbf{h}_t \\in \\mathbb{R}^h$ \nand the output $\\mathbf{o}_t \\in \\mathbb{R}^q$\nare computed as\n\n$$\\begin{aligned}\\mathbf{h}_t &= \\mathbf{W}_\\textrm{hx} \\mathbf{x}_t + \\mathbf{W}_\\textrm{hh} \\mathbf{h}_{t-1},\\\\\n\\mathbf{o}_t &= \\mathbf{W}_\\textrm{qh} \\mathbf{h}_{t},\\end{aligned}$$\n\nwhere $\\mathbf{W}_\\textrm{hx} \\in \\mathbb{R}^{h \\times d}$, $\\mathbf{W}_\\textrm{hh} \\in \\mathbb{R}^{h \\times h}$, and\n$\\mathbf{W}_\\textrm{qh} \\in \\mathbb{R}^{q \\times h}$\nare the weight parameters.\nDenote by $l(\\mathbf{o}_t, y_t)$\nthe loss at time step $t$. \nOur objective function,\nthe loss over $T$ time steps\nfrom the beginning of the sequence is thus\n\n$$L = \\frac{1}{T} \\sum_{t=1}^T l(\\mathbf{o}_t, y_t).$$\n\n\nIn order to visualize the dependencies among\nmodel variables and parameters during computation\nof the RNN,\nwe can draw a computational graph for the model,\nas shown in :numref:`fig_rnn_bptt`.\nFor example, the computation of the hidden states of time step 3,\n$\\mathbf{h}_3$, depends on the model parameters\n$\\mathbf{W}_\\textrm{hx}$ and $\\mathbf{W}_\\textrm{hh}$,\nthe hidden state of the previous time step $\\mathbf{h}_2$,\nand the input of the current time step $\\mathbf{x}_3$.\n\n![image.png](attachment:70a0a32b-13a6-402c-bb22-0a2042350300.png)\n:label:`fig_rnn_bptt`\n\nAs just mentioned, the model parameters in :numref:`fig_rnn_bptt` \nare $\\mathbf{W}_\\textrm{hx}$, $\\mathbf{W}_\\textrm{hh}$, and $\\mathbf{W}_\\textrm{qh}$. \nGenerally, training this model requires \ngradient computation with respect to these parameters\n$\\partial L/\\partial \\mathbf{W}_\\textrm{hx}$, $\\partial L/\\partial \\mathbf{W}_\\textrm{hh}$, and $\\partial L/\\partial \\mathbf{W}_\\textrm{qh}$.\nAccording to the dependencies in :numref:`fig_rnn_bptt`,\nwe can traverse in the opposite direction of the arrows\nto calculate and store the gradients in turn.\nTo flexibly express the multiplication of \nmatrices, vectors, and scalars of different shapes\nin the chain rule,\nwe continue to use the $\\textrm{prod}$ operator \nas described in :numref:`sec_backprop`.\n\n\nFirst of all, differentiating the objective function\nwith respect to the model output at any time step $t$\nis fairly straightforward:\n\n$$\\frac{\\partial L}{\\partial \\mathbf{o}_t} =  \\frac{\\partial l (\\mathbf{o}_t, y_t)}{T \\cdot \\partial \\mathbf{o}_t} \\in \\mathbb{R}^q.$$\n:eqlabel:`eq_bptt_partial_L_ot`\n\nNow we can calculate the gradient of the objective \nwith respect to the parameter $\\mathbf{W}_\\textrm{qh}$\nin the output layer:\n$\\partial L/\\partial \\mathbf{W}_\\textrm{qh} \\in \\mathbb{R}^{q \\times h}$. \nBased on :numref:`fig_rnn_bptt`, \nthe objective $L$ depends on $\\mathbf{W}_\\textrm{qh}$ \nvia $\\mathbf{o}_1, \\ldots, \\mathbf{o}_T$. \nUsing the chain rule yields\n\n$$\n\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{qh}}\n= \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{W}_\\textrm{qh}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{o}_t} \\mathbf{h}_t^\\top,\n$$\n\nwhere $\\partial L/\\partial \\mathbf{o}_t$\nis given by :eqref:`eq_bptt_partial_L_ot`.\n\nNext, as shown in :numref:`fig_rnn_bptt`,\nat the final time step $T$,\nthe objective function\n$L$ depends on the hidden state $\\mathbf{h}_T$ \nonly via $\\mathbf{o}_T$.\nTherefore, we can easily find the gradient \n$\\partial L/\\partial \\mathbf{h}_T \\in \\mathbb{R}^h$\nusing the chain rule:\n\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_T} = \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_T}, \\frac{\\partial \\mathbf{o}_T}{\\partial \\mathbf{h}_T} \\right) = \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_T}.$$\n:eqlabel:`eq_bptt_partial_L_hT_final_step`\n\nIt gets trickier for any time step $t < T$,\nwhere the objective function $L$ depends on \n$\\mathbf{h}_t$ via $\\mathbf{h}_{t+1}$ and $\\mathbf{o}_t$.\nAccording to the chain rule,\nthe gradient of the hidden state\n$\\partial L/\\partial \\mathbf{h}_t \\in \\mathbb{R}^h$\nat any time step $t < T$ can be recurrently computed as:\n\n\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}, \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} \\right) + \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} \\right) = \\mathbf{W}_\\textrm{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} + \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}.$$\n:eqlabel:`eq_bptt_partial_L_ht_recur`\n\nFor analysis, expanding the recurrent computation\nfor any time step $1 \\leq t \\leq T$ gives\n\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_\\textrm{hh}^\\top\\right)}^{T-i} \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_{T+t-i}}.$$\n:eqlabel:`eq_bptt_partial_L_ht`\n\nWe can see from :eqref:`eq_bptt_partial_L_ht` \nthat this simple linear example already\nexhibits some key problems of long sequence models:\nit involves potentially very large powers of $\\mathbf{W}_\\textrm{hh}^\\top$.\nIn it, eigenvalues smaller than 1 vanish\nand eigenvalues larger than 1 diverge.\nThis is numerically unstable,\nwhich manifests itself in the form of vanishing \nand exploding gradients.\nOne way to address this is to truncate the time steps\nat a computationally convenient size \nas discussed in :numref:`subsec_bptt_analysis`. \nIn practice, this truncation can also be effected \nby detaching the gradient after a given number of time steps.\nLater on, we will see how more sophisticated sequence models \nsuch as long short-term memory can alleviate this further. \n\nFinally, :numref:`fig_rnn_bptt` shows \nthat the objective function $L$ \ndepends on model parameters $\\mathbf{W}_\\textrm{hx}$ and $\\mathbf{W}_\\textrm{hh}$\nin the hidden layer via hidden states\n$\\mathbf{h}_1, \\ldots, \\mathbf{h}_T$.\nTo compute gradients with respect to such parameters\n$\\partial L / \\partial \\mathbf{W}_\\textrm{hx} \\in \\mathbb{R}^{h \\times d}$ and $\\partial L / \\partial \\mathbf{W}_\\textrm{hh} \\in \\mathbb{R}^{h \\times h}$,\nwe apply the chain rule giving\n\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{hx}}\n&= \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_\\textrm{hx}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{x}_t^\\top,\\\\\n\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{hh}}\n&= \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_\\textrm{hh}}\\right)\n= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{h}_{t-1}^\\top,\n\\end{aligned}\n$$\n\nwhere $\\partial L/\\partial \\mathbf{h}_t$\nwhich is recurrently computed by\n:eqref:`eq_bptt_partial_L_hT_final_step`\nand :eqref:`eq_bptt_partial_L_ht_recur`\nis the key quantity that affects the numerical stability.\n\n\n\nSince backpropagation through time is the application of backpropagation in RNNs,\nas we have explained in :numref:`sec_backprop`,\ntraining RNNs alternates forward propagation with\nbackpropagation through time.\nMoreover, backpropagation through time\ncomputes and stores the above gradients in turn.\nSpecifically, stored intermediate values\nare reused to avoid duplicate calculations,\nsuch as storing $\\partial L/\\partial \\mathbf{h}_t$\nto be used in computation of both $\\partial L / \\partial \\mathbf{W}_\\textrm{hx}$ \nand $\\partial L / \\partial \\mathbf{W}_\\textrm{hh}$.\n\n\n### Summary\n\nBackpropagation through time is merely an application of backpropagation to sequence models with a hidden state.\nTruncation, such as regular or randomized, is needed for computational convenience and numerical stability.\nHigh powers of matrices can lead to divergent or vanishing eigenvalues. This manifests itself in the form of exploding or vanishing gradients.\nFor efficient computation, intermediate values are cached during backpropagation through time.\n\n\n","metadata":{},"attachments":{"70a0a32b-13a6-402c-bb22-0a2042350300.png":{"image/png":"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"},"8c71f03c-a510-4ac3-8bb0-657f35767c6a.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"## My Note for 9.7.2\nSection 9.7.2, titled \"Backpropagation Through Time in Detail,\" moves from the general principles of computing gradients in Recurrent Neural Networks (RNNs) to a more specific, mathematical analysis of how these gradients are calculated with respect to the decomposed model parameters. This detailed breakdown is crucial for understanding the **vanishing and exploding gradient problems** in RNNs and the rationale behind practical solutions like gradient clipping and truncation.\n\nTo simplify the analysis, the sources consider an RNN model with the following characteristics:\n*   **No bias parameters**.\n*   The **activation function in the hidden layer is an identity mapping** ($\\phi(x) = x$).\n*   It focuses on a **single example input** at each time step.\n\nLet's break down the content and formulas:\n\n### Simplified RNN Model and Objective Function\n\n1.  **Hidden State and Output Computation (9.7.9)**:\n    At each time step $t$, the hidden state $\\mathbf{h}_t \\in \\mathbb{R}^h$ and the output $\\mathbf{o}_t \\in \\mathbb{R}^q$ are computed as follows:\n    $\\mathbf{h}_t = \\mathbf{W}_\\textrm{hx} \\mathbf{x}_t + \\mathbf{W}_\\textrm{hh} \\mathbf{h}_{t-1}$\n    $\\mathbf{o}_t = \\mathbf{W}_\\textrm{qh} \\mathbf{h}_{t}$\n\n    *   **$\\mathbf{x}_t \\in \\mathbb{R}^d$**: The input at time step $t$.\n    *   **$\\mathbf{y}_t$**: The desired target (label) at time step $t$.\n    *   **$\\mathbf{h}_t \\in \\mathbb{R}^h$**: The hidden state at time step $t$, where $h$ is the number of hidden units. This state captures and retains the sequence's historical information up to the current time step.\n    *   **$\\mathbf{o}_t \\in \\mathbb{R}^q$**: The output at time step $t$, where $q$ is the output dimension.\n    *   **$\\mathbf{W}_\\textrm{hx} \\in \\mathbb{R}^{h \\times d}$**: The weight parameter connecting the current input $\\mathbf{x}_t$ to the hidden state $\\mathbf{h}_t$.\n    *   **$\\mathbf{W}_\\textrm{hh} \\in \\mathbb{R}^{h \\times h}$**: The weight parameter describing how the hidden state from the previous time step $\\mathbf{h}_{t-1}$ influences the current hidden state $\\mathbf{h}_t$. This is the **recurrent connection**.\n    *   **$\\mathbf{W}_\\textrm{qh} \\in \\mathbb{R}^{q \\times h}$**: The weight parameter connecting the hidden state $\\mathbf{h}_t$ to the output $\\mathbf{o}_t$.\n\n    **Derivation/Explanation**: This formula defines the forward pass of a simplified RNN. The current hidden state $\\mathbf{h}_t$ is a linear combination of the current input $\\mathbf{x}_t$ (transformed by $\\mathbf{W}_\\textrm{hx}$) and the previous hidden state $\\mathbf{h}_{t-1}$ (transformed by $\\mathbf{W}_\\textrm{hh}$). The output $\\mathbf{o}_t$ is then a linear transformation of the current hidden state $\\mathbf{h}_t$. This simplified model with identity activation makes the derivatives simpler for analytical purposes, but the core principles of dependency apply to more complex activation functions like tanh.\n\n2.  **Objective Function (Loss) (9.7.10)**:\n    The objective function $L$, representing the loss over $T$ time steps from the beginning of the sequence, is defined as:\n    $L = \\frac{1}{T} \\sum_{t=1}^T l(\\mathbf{o}_t, \\mathbf{y}_t)$\n\n    *   **$l(\\mathbf{o}_t, \\mathbf{y}_t)$**: The loss at a specific time step $t$, comparing the model's output $\\mathbf{o}_t$ with the desired target $\\mathbf{y}_t$.\n\n    **Derivation/Explanation**: This is the standard way to define the total loss for sequence models: summing (or averaging, as here) the losses computed at each individual time step. This means that errors at later time steps can influence the gradients throughout the entire sequence back to the beginning.\n\n### Visualizing Dependencies: Fig. 9.7.2\n**Fig. 9.7.2** illustrates the computational graph for an RNN model with three time steps.\n*   It visually shows how each hidden state $\\mathbf{h}_t$ depends on the input $\\mathbf{x}_t$, the previous hidden state $\\mathbf{h}_{t-1}$, and the weight parameters $\\mathbf{W}_\\textrm{hx}$ and $\\mathbf{W}_\\textrm{hh}$.\n*   It also shows how each output $\\mathbf{o}_t$ depends on $\\mathbf{h}_t$ and $\\mathbf{W}_\\textrm{qh}$.\n*   This graph is essential for understanding **Backpropagation Through Time (BPTT)**, as it shows the dependencies that need to be traversed in reverse to compute gradients.\n\n### Gradient Computation in Detail\n\nTraining this RNN model requires computing gradients with respect to all its parameters: $\\partial L/\\partial \\mathbf{W}_\\textrm{hx}$, $\\partial L/\\partial \\mathbf{W}_\\textrm{hh}$, and $\\partial L/\\partial \\mathbf{W}_\\textrm{qh}$. This involves traversing the computational graph in the opposite direction of the arrows, applying the **chain rule** and reusing intermediate values to avoid duplicate calculations. The $\\textrm{prod}$ operator is used to flexibly express multiplication of matrices, vectors, and scalars of different shapes.\n\n1.  **Gradient of $L$ with respect to Output $\\mathbf{o}_t$ (9.7.11)**:\n    $\\frac{\\partial L}{\\partial \\mathbf{o}_t} = \\frac{\\partial l (\\mathbf{o}_t, \\mathbf{y}_t)}{T \\cdot \\partial \\mathbf{o}_t} \\in \\mathbb{R}^q$\n\n    **Derivation/Explanation**: This is the gradient of the overall objective function $L$ with respect to the output at a specific time step $\\mathbf{o}_t$. Since $L$ is the average of individual losses $l(\\mathbf{o}_t, \\mathbf{y}_t)$ over $T$ time steps, the derivative of $L$ with respect to $\\mathbf{o}_t$ is simply the derivative of $l(\\mathbf{o}_t, \\mathbf{y}_t)$ with respect to $\\mathbf{o}_t$, scaled by $1/T$.\n\n2.  **Gradient of $L$ with respect to Output Layer Weights $\\mathbf{W}_\\textrm{qh}$ (9.7.12)**:\n    $\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{qh}} = \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{W}_\\textrm{qh}}\\right) = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{o}_t} \\mathbf{h}_t^\\top$\n\n    **Derivation/Explanation**: The objective $L$ depends on $\\mathbf{W}_\\textrm{qh}$ through all outputs $\\mathbf{o}_1, \\ldots, \\mathbf{o}_T$. Therefore, we sum the contributions from each time step. For a single time step $t$, from the output equation $\\mathbf{o}_t = \\mathbf{W}_\\textrm{qh} \\mathbf{h}_t$, the derivative of $\\mathbf{o}_t$ with respect to $\\mathbf{W}_\\textrm{qh}$ is $\\mathbf{h}_t^\\top$. Applying the chain rule, $\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{qh}}$ is the sum of $(\\frac{\\partial L}{\\partial \\mathbf{o}_t}) \\mathbf{h}_t^\\top$ for all $t$.\n\n3.  **Gradient of $L$ with respect to Hidden State at Final Time Step $\\mathbf{h}_T$ (9.7.13)**:\n    $\\frac{\\partial L}{\\partial \\mathbf{h}_T} = \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_T}, \\frac{\\partial \\mathbf{o}_T}{\\partial \\mathbf{h}_T} \\right) = \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_T}$\n\n    **Derivation/Explanation**: At the final time step $T$, the objective function $L$ depends on the hidden state $\\mathbf{h}_T$ *only* via the output $\\mathbf{o}_T$. From $\\mathbf{o}_T = \\mathbf{W}_\\textrm{qh} \\mathbf{h}_T$, the derivative of $\\mathbf{o}_T$ with respect to $\\mathbf{h}_T$ is $\\mathbf{W}_\\textrm{qh}$. Using the chain rule, $\\frac{\\partial L}{\\partial \\mathbf{h}_T}$ is $\\mathbf{W}_\\textrm{qh}^\\top$ multiplied by $\\frac{\\partial L}{\\partial \\mathbf{o}_T}$.\n\n4.  **Gradient of $L$ with respect to Hidden State at Time Step $t < T$ (9.7.14)**:\n    $\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}, \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} \\right) + \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} \\right) = \\mathbf{W}_\\textrm{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} + \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}$\n\n    **Derivation/Explanation**: This is the **recurrent part of backpropagation**. For any time step $t < T$, the objective function $L$ depends on $\\mathbf{h}_t$ through two paths:\n    *   **Direct path to current output**: Through $\\mathbf{o}_t$, which contributes to $l(\\mathbf{o}_t, \\mathbf{y}_t)$ and thus to $L$. From $\\mathbf{o}_t = \\mathbf{W}_\\textrm{qh} \\mathbf{h}_t$, the derivative $\\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} = \\mathbf{W}_\\textrm{qh}$. So, this path contributes $\\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}$.\n    *   **Recurrent path to future hidden states**: Through $\\mathbf{h}_{t+1}$ (and all subsequent hidden states and outputs). From $\\mathbf{h}_{t+1} = \\mathbf{W}_\\textrm{hx} \\mathbf{x}_{t+1} + \\mathbf{W}_\\textrm{hh} \\mathbf{h}_t$, the derivative $\\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} = \\mathbf{W}_\\textrm{hh}$. So, this path contributes $\\mathbf{W}_\\textrm{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}$.\n    The chain rule sums these two contributions to get the total gradient $\\frac{\\partial L}{\\partial \\mathbf{h}_t}$.\n\n5.  **Expanded Gradient for Hidden State $\\mathbf{h}_t$ (9.7.15)**:\n    $\\frac{\\partial L}{\\partial \\mathbf{h}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_\\textrm{hh}^\\top\\right)}^{T-i} \\mathbf{W}_\\textrm{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_{T+t-i}}$\n\n    **Derivation/Explanation**: This formula is obtained by **recursively expanding equation (9.7.14)**. It shows that the gradient with respect to $\\mathbf{h}_t$ depends on a sum of terms, where each term involves powers of $\\mathbf{W}_\\textrm{hh}^\\top$.\n    *   The term $(\\mathbf{W}_\\textrm{hh}^\\top)^{T-i}$ signifies that the influence of $\\mathbf{h}_t$ on a future hidden state $\\mathbf{h}_i$ (and thus on its loss contribution) involves $T-i$ multiplications by $\\mathbf{W}_\\textrm{hh}^\\top$ as the gradient backpropagates through time.\n    *   **Significance for Vanishing/Exploding Gradients**: This explicit form reveals the **fundamental cause of numerical instability** in RNNs.\n        *   If the eigenvalues of $\\mathbf{W}_\\textrm{hh}^\\top$ (or $\\mathbf{W}_\\textrm{hh}$) are consistently **smaller than 1**, repeated multiplication by these matrices causes the gradient terms to **vanish** (approach zero). This makes it difficult for the model to learn long-term dependencies.\n        *   If the eigenvalues are consistently **larger than 1**, repeated multiplication causes the gradient terms to **explode** (become excessively large). This can lead to unstable training, huge parameter updates, and divergence.\n    This problem is often addressed by **truncating time steps** (limiting how far back gradients are propagated) or using more sophisticated architectures like LSTMs.\n\n6.  **Gradients with respect to Hidden Layer Weights $\\mathbf{W}_\\textrm{hx}$ and $\\mathbf{W}_\\textrm{hh}$ (9.7.16)**:\n    $\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{hx}} = \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_\\textrm{hx}}\\right) = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{x}_t^\\top$\n    $\\frac{\\partial L}{\\partial \\mathbf{W}_\\textrm{hh}} = \\sum_{t=1}^T \\textrm{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_\\textrm{hh}}\\right) = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{h}_{t-1}^\\top$\n\n    **Derivation/Explanation**: The objective function $L$ depends on $\\mathbf{W}_\\textrm{hx}$ and $\\mathbf{W}_\\textrm{hh}$ via all hidden states $\\mathbf{h}_1, \\ldots, \\mathbf{h}_T$. Similar to $\\mathbf{W}_\\textrm{qh}$, we sum the contributions from each time step:\n    *   For $\\mathbf{W}_\\textrm{hx}$: From $\\mathbf{h}_t = \\mathbf{W}_\\textrm{hx} \\mathbf{x}_t + \\mathbf{W}_\\textrm{hh} \\mathbf{h}_{t-1}$, the derivative of $\\mathbf{h}_t$ with respect to $\\mathbf{W}_\\textrm{hx}$ is $\\mathbf{x}_t^\\top$.\n    *   For $\\mathbf{W}_\\textrm{hh}$: From $\\mathbf{h}_t = \\mathbf{W}_\\textrm{hx} \\mathbf{x}_t + \\mathbf{W}_\\textrm{hh} \\mathbf{h}_{t-1}$, the derivative of $\\mathbf{h}_t$ with respect to $\\mathbf{W}_\\textrm{hh}$ is $\\mathbf{h}_{t-1}^\\top$.\n    These gradients depend critically on $\\frac{\\partial L}{\\partial \\mathbf{h}_t}$, which is computed recurrently by (9.7.13) and (9.7.14) and is the **key quantity affecting numerical stability**. During BPTT, these intermediate $\\frac{\\partial L}{\\partial \\mathbf{h}_t}$ values are cached and reused for efficient computation.\n\n### Summary of Backpropagation Through Time\n\nIn essence, Backpropagation Through Time (BPTT) is the direct application of the backpropagation algorithm to unrolled RNN computational graphs. It involves:\n*   **Unrolling the RNN**: Expanding the network across time steps, treating it like a deep feedforward network where parameters are shared across layers (time steps).\n*   **Applying the Chain Rule**: Gradients are computed by traversing the unrolled graph backward from the output to the input.\n*   **Summing Gradients**: The gradient for each parameter (e.g., $\\mathbf{W}_\\textrm{hh}$) must be summed across all time steps where it appears.\n*   **Numerical Instability**: The long chains of matrix products, especially involving powers of $\\mathbf{W}_\\textrm{hh}^\\top$, lead to **vanishing or exploding gradients**, making training difficult for long sequences.\n*   **Practical Solutions**: **Truncation** (regular or randomized) is used for computational convenience and to mitigate numerical instability by limiting the length of backpropagation paths. **Gradient clipping** is another ubiquitous heuristic that directly addresses exploding gradients by limiting their magnitude.","metadata":{}}]}